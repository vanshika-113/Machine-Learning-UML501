{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e60729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.0001 | Cost: 9249.4203 | R2: -1.8360\n",
      "LR: 0.001  | Cost: 1404.4974 | R2: 0.5549\n",
      "LR: 0.01   | Cost: 17.7321 | R2: 0.9947\n",
      "LR: 0.1    | Cost: 15.5167 | R2: 0.9954\n",
      "LR: 1      | Model Failed or Diverged (ValueError)\n",
      "LR: 10     | Model Failed or Diverged (ValueError)\n",
      "\n",
      "---------------------------------------------------\n",
      "No stable model found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_12444\\3676403224.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "  grad_features=(1/m)*(X[:,1:].T@errors)+(self.lambda_param * self.theta[1:])\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_12444\\3676403224.py:32: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X@self.theta\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_correlated_data(n_samples=1000,n_features=7,random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    X_base=np.random.rand(n_samples,1)*10\n",
    "    X=X_base.copy()\n",
    "    for i in range(n_features-1):\n",
    "        X_correlated=X_base*(1+(i+1)*0.5)+np.random.normal(0,0.5,(n_samples,1))\n",
    "        X=np.hstack((X,X_correlated))\n",
    "    true_weights=np.array([5,-2,1.5,3,-1,0.5,4])\n",
    "    bias=10\n",
    "    y=X@true_weights+bias+np.random.normal(0,5,n_samples)\n",
    "    return X,y\n",
    "X,y=generate_correlated_data()\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "X_b=np.c_[np.ones((X_scaled.shape[0],1)),X_scaled]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "    X_b,y,test_size=0.3,random_state=42\n",
    ")\n",
    "class RidgeRegressionGD:\n",
    "    def __init__(self,learning_rate=0.01,n_iterations=1000,lambda_param=1e-5):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_iterations=n_iterations\n",
    "        self.lambda_param=lambda_param\n",
    "        self.theta=None\n",
    "    def hypothesis(self,X):\n",
    "        return X@self.theta\n",
    "    def cost_function(self,X,y):\n",
    "        m=len(y)\n",
    "        mse=(1/(2*m))*np.sum((self.hypothesis(X)-y)**2)\n",
    "        regularization=self.lambda_param*np.sum(self.theta[1:]**2)\n",
    "        return mse+regularization\n",
    "    def fit(self,X,y):\n",
    "        m,n=X.shape\n",
    "        self.theta=np.zeros(n)\n",
    "        for _ in range(self.n_iterations):\n",
    "            predictions=self.hypothesis(X)\n",
    "            errors=predictions-y\n",
    "            grad_0=(1/m)*np.sum(errors*X[:,0])\n",
    "            grad_features=(1/m)*(X[:,1:].T@errors)+(self.lambda_param * self.theta[1:])\n",
    "            self.theta[0]-=self.learning_rate * grad_0\n",
    "            self.theta[1:]-=self.learning_rate * grad_features\n",
    "        return self\n",
    "    def predict(self,X):\n",
    "        return self.hypothesis(X)\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lambda_param = 1e-5  \n",
    "results = []\n",
    "best_r2 = -np.inf\n",
    "best_cost = np.inf\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = RidgeRegressionGD(learning_rate=lr, n_iterations=1000, lambda_param=lambda_param)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        if not np.all(np.isfinite(y_pred_test)):\n",
    "            raise ValueError(\"Model Diverged: Predictions contain NaN/inf.\")\n",
    "            \n",
    "        \n",
    "        train_cost = model.cost_function(X_train, y_train)\n",
    "        r2 = r2_score(y_test, y_pred_test) \n",
    "        \n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'lambda': lambda_param,\n",
    "            'Train Cost': train_cost,\n",
    "            'R2 Score': r2\n",
    "        })\n",
    "\n",
    "        print(f\"LR: {lr:<6} | Cost: {train_cost:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    except (OverflowError, ValueError) as e:\n",
    "        \n",
    "        print(f\"LR: {lr:<6} | Model Failed or Diverged ({type(e).__name__})\")\n",
    "        results.append({'lr': lr, 'lambda': lambda_param, 'Train Cost': np.nan, 'R2 Score': np.nan})\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\n---------------------------------------------------\")\n",
    "if best_params:\n",
    "    print(f\"Best Parameters Found:\")\n",
    "    print(f\"Learning Rate (LR): {best_params['lr']}\")\n",
    "    print(f\"Minimum Ridge Cost (Train): {best_params['cost']:.4f}\")\n",
    "    print(f\"Maximum R2 Score (Test): {best_params['r2']:.4f}\")\n",
    "else:\n",
    "    print(\"No stable model found.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1a5304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "483c0eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                 |         MSE |     RMSE |   R2 Score |\n",
      "|:----------------------|------------:|---------:|-----------:|\n",
      "| Ridge Regression (L2) | 145189.5482 | 381.0375 |     0.4021 |\n",
      "| LASSO Regression (L1) | 146328.8781 | 382.5296 |     0.3974 |\n",
      "| Linear Regression     | 150406.5837 | 387.8229 |     0.3806 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\Hitters (1).csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.dropna(subset=['Salary'], inplace=True) \n",
    "\n",
    "df.dropna(inplace=True) \n",
    "\n",
    "\n",
    "X = df.drop('Salary', axis=1)\n",
    "y = df['Salary']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        \n",
    "        ('num', StandardScaler(), numerical_features), \n",
    "        \n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "alpha_val = 0.5748 \n",
    "\n",
    "\n",
    "linear_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', LinearRegression())])\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "ridge_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('regressor', Ridge(alpha=alpha_val, random_state=42))])\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "lasso_model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('regressor', Lasso(alpha=alpha_val, random_state=42, max_iter=10000))])\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': linear_model,\n",
    "    'Ridge Regression (L2)': ridge_model,\n",
    "    'LASSO Regression (L1)': lasso_model\n",
    "}\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    evaluation_results.append({'Model': name, 'MSE': mse, 'RMSE': rmse, 'R2 Score': r2})\n",
    "\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "\n",
    "\n",
    "print(results_df.sort_values(by='R2 Score', ascending=False).to_markdown(index=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512b3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Cross Validation (RidgeCV)\n",
      "Optimal Alpha (Lambda) for RidgeCV: 10.7227\n",
      "RidgeCV Test R2 Score: 0.7071\n",
      "\n",
      " Lasso Cross Validation (LassoCV)\n",
      "Optimal Alpha (Lambda) for LassoCV: 0.0070\n",
      "LassoCV Test R2 Score: 0.7137\n",
      "Number of non-zero coefficients (Feature Selection): 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1622: FutureWarning: 'n_alphas' was deprecated in 1.7 and will be removed in 1.9. 'alphas' now accepts an integer value which removes the need to pass 'n_alphas'. The default value of 'alphas' will change from None to 100 in 1.9. Pass an explicit value to 'alphas' and leave 'n_alphas' to its default value to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "\n",
    "#print(\"Loading Boston Housing Dataset\")\n",
    "try:\n",
    "    \n",
    "    boston = fetch_openml(name='boston', version=1, as_frame=True, parser='auto')\n",
    "    X = boston.data\n",
    "    y = boston.target\n",
    "    \n",
    "    \n",
    "    if X.isnull().any().any():\n",
    "        print(\"Handling missing values (Imputing mean for numerical data).\")\n",
    "        \n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading Boston dataset: {e}. Cannot proceed with Q3.\")\n",
    "    \n",
    "\n",
    "\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist() \n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        \n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        \n",
    "        \n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "alphas = np.logspace(-4, 2, 100) \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nRidge Cross Validation (RidgeCV)\")\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5) # 5-fold CV\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_alpha_ridge = ridge_cv.alpha_\n",
    "r2_ridge = ridge_cv.score(X_test, y_test)\n",
    "\n",
    "print(f\"Optimal Alpha (Lambda) for RidgeCV: {best_alpha_ridge:.4f}\")\n",
    "print(f\"RidgeCV Test R2 Score: {r2_ridge:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Lasso Cross Validation (LassoCV)\")\n",
    "\n",
    "lasso_cv = LassoCV(n_alphas=100, cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_alpha_lasso = lasso_cv.alpha_\n",
    "r2_lasso = lasso_cv.score(X_test, y_test)\n",
    "non_zero_coeffs = np.sum(lasso_cv.coef_ != 0)\n",
    "\n",
    "print(f\"Optimal Alpha (Lambda) for LassoCV: {best_alpha_lasso:.4f}\")\n",
    "print(f\"LassoCV Test R2 Score: {r2_lasso:.4f}\")\n",
    "print(f\"Number of non-zero coefficients (Feature Selection): {non_zero_coeffs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6927051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0 (Class setosa vs. Rest) trained.\n",
      "Classifier 1 (Class versicolor vs. Rest) trained.\n",
      "Classifier 2 (Class virginica vs. Rest) trained.\n",
      "Multiclass Logistic Regression (OvR) Test Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "n_classes = len(np.unique(y)) \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The Sigmoid function.\"\"\"\n",
    "    \n",
    "    z = np.clip(z, -500, 500) \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent_binary(X, y, lr, n_iterations):\n",
    "    \"\"\"Gradient Descent for a single Binary Logistic Regression classifier.\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n) \n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        z = X @ theta\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "       \n",
    "        gradient = (1 / m) * (X.T @ (h - y))\n",
    "        \n",
    "        \n",
    "        theta = theta - lr * gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "n_iterations = 1000\n",
    "n_features = X_train_b.shape[1]\n",
    "\n",
    "\n",
    "theta_ovr = np.zeros((n_classes, n_features))\n",
    "\n",
    "\n",
    "for k in range(n_classes): \n",
    "    \n",
    "    y_binary = (y_train == k).astype(int)\n",
    "    \n",
    "    \n",
    "    theta_ovr[k, :] = gradient_descent_binary(X_train_b, y_binary, lr, n_iterations)\n",
    "    \n",
    "    print(f\"Classifier {k} (Class {iris.target_names[k]} vs. Rest) trained.\")\n",
    "\n",
    "\n",
    "\n",
    "def predict_ovr(X, theta_ovr):\n",
    "    \"\"\"Predicts class labels using the OvR strategy.\"\"\"\n",
    "    \n",
    "    scores = X @ theta_ovr.T\n",
    "    \n",
    "    \n",
    "    return np.argmax(scores, axis=1)\n",
    "\n",
    "\n",
    "y_pred_test = predict_ovr(X_test_b, theta_ovr)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "print(f\"Multiclass Logistic Regression (OvR) Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
